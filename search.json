[
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "This is where you describe how to get set up on a clean install, including the commands necessary to get the raw data (using the sync_data_from_s3 command, for example), and then how to make the cleaned, final data sets."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "docs1",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "docs1",
    "section": "Description",
    "text": "Description\nSolve for committor function using neural networks."
  },
  {
    "objectID": "index.html#commands",
    "href": "index.html#commands",
    "title": "docs1",
    "section": "Commands",
    "text": "Commands\nThe Makefile contains the central entry points for common tasks related to this project.\n\nSyncing data to cloud storage\n\nmake sync_data_up will use aws s3 sync to recursively sync files in data/ up to s3://maier-stein-bucket/data/.\nmake sync_data_down will use aws s3 sync to recursively sync files from s3://maier-stein-bucket/data/ to data/."
  },
  {
    "objectID": "index.html#philisophical-points",
    "href": "index.html#philisophical-points",
    "title": "docs1",
    "section": "Philisophical Points",
    "text": "Philisophical Points\nIn a typical machine learning pipeline, datasets, either labeled or unlabeled, are generated in one file, then passed to another file where the training happens. The separation of dataset code and training code is reflected in the cookie-cutter project organization: dataset.py and modeling/train.py are two separate files.\nFor the Maier Stein solver, there are two types of datasets at play. The first kind are unlabeled datasets consisting of points in the transition region. These are unlabeled because we do not know the value of the committor function at such points. The second kind of dataset are labeled datasets consisting of points \\(x \\in \\partial A\\) labeled by zero and points \\(x \\in \\partial B\\) labeled by one. The logic for this labeling comes from the meaning of the committor function.\nOne way we could have set up the machine learning pipeline is as follows. First, we merge the unlabeled and labeled data together into a tuple. Then we pass this (labeled, unlabeled) tuple to the neural network training program. We did not do it this way. Even while we experiment with different datasets for the transition region, the boundary data remains constant. This reflects the idea that the boundary data is a fixed property of the system PDE, rather than a variable entity. On the other hand, we are interested in experimenting with different datasets for the transition region. As such, it would have made sense to actually generate the boundary dataset inside the train.py file. However, in the end, we chose not to do this because one might like to experiment with other boundary datasets in the future, and there is also just the satisfaction of all datasets being generated from the dataset.py file."
  },
  {
    "objectID": "index.html#mathematical-discussion",
    "href": "index.html#mathematical-discussion",
    "title": "docs1",
    "section": "Mathematical Discussion",
    "text": "Mathematical Discussion\nNote that I am not an expert on the following matters, so I welcome any corrections, as I will not perform any rigrous deductions.\nJust as a function \\(f(t, x)\\) can evolve in time, so too can a probability distribution. Further, so too can a conditional probability distribution evolve in time. In notation, we write \\(p_t(x,dy)\\), and this means the conitional probability density of transitioning to the infinitisimal region \\(dy\\), given that we are already located at point \\(x\\). Oh, and don’t forget that the time is \\(t\\). So presumably, at another time \\(s\\), the transition probability could look different, even if the point \\(x\\) and the infinitisemal region \\(dy\\) remain the same."
  },
  {
    "objectID": "notes-by-date.html",
    "href": "notes-by-date.html",
    "title": "Developer Notes",
    "section": "",
    "text": "December 31, 2025\nOne reason that the /data/ folder is in the .gitignore file is that s3 will be used for storing data, so it doesn’t make sense to exchange data back and forth between the github and the local computer. Cookie cutter data science calls this idea “separating the codebase from the data” https://drivendata.co/blog/ccds-v2#data-storage\nHere are other key resources from today: https://git-scm.com/docs/git-credential and https://git-scm.com/docs/gitfaq#_credentials. The latter link has the question: how do I specify my credentials when pushing over https?\n\n\nJanuary 2, 2025\nI am checking if Github Actions actually publishes and deploys my website at the link: https://williamclarktennis.github.io/MaierSteinSolverRepo/.\nWhy is it not working? Please work!\n\n\nJanuary 9, 2026\nI did a simple example with Python logging to gain some motivation for loguru, present in the dataset.py file and other files.\nSee https://docs.python.org/3/library/pathlib.html to learn about PROJ_ROOT = Path(__file__).resolve().parents[1] in the config.py file.\nI’m currently uncomfortable with the following files and items:\n\nthe .env file. It is currently empty. What other files depend on it? I’m afraid I’ll have to understand this file before beginning development of source code.\nThe Makefile. Hopefully this file can be ignored while I develop the source code.\nThe pyproject.toml file. I think this can be ignored while I develop source code.\nThe MaierSteinSolver/config.py file. It says to load environment variables from the .env file, but I don’t know what those would be yet.\nThe dataset.py file. When I want to develop source code, should my code go into the main() function? I’m not sure how typer works, so I can’t answer this question yet. Hopefully once I get a feeling for typer, then I can start putting source code into the dataset.py file."
  }
]